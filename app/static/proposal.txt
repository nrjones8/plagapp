Plagiarism "CopyCats" Comps
Introduction

Plagiarism in higher education is a prevalent issue. In one study of plagiarism among university undergraduates, 40% of the students surveyed admitted to copying sentences into written assignments without attribution (Gabriel, 2010). Our goal is to produce a user-friendly piece of software which, given a text, shows the user sections of the text which may be plagiarised. The program will also output its confidence and reason for indicating each section. We will use two broad approaches to plagiarism detection, intrinsic and extrinsic, which are described in subsequent sections.

Intrinsic plagiarism detection

Intrinsic plagiarism detection is the practice of detecting plagiarism within a document without the use of any outside reference documents. Intrinsic plagiarism detection is useful when sources of a potentially plagiarised document are inaccessible, rendering extrinsic approaches ineffective. The majority of today’s research into intrinsic plagiarism detection has focused on the concept that authors have unique writing styles that can be captured by analyzing stylometric features present within passages of a given text. Stylometric features are chosen in such a way that they capture authors’ unique writing styles. Features such as "average sentence length" and "frequency of rare vocabulary" are used to predict which passages may have been written by different authors. The methodology used in our approach will likely be very similar to the process outlined in J. Novak, P. Raghavan, and A. Tomkins’s "Anti-Aliasing on the Web" paper (2004) (Novak, 2004). The idea is to break a document into a subset of passages and analyze the stylometric features of each passage.  Any inconsistencies could indicate plagiarism.

Our preliminary focus when developing our own intrinsic plagiarism detection system will be to analyze passages strictly on words and word choice because it has been shown to be a reliable measure of authorship (Novak, 2004). Once separated, passages will be clustered with other passages that have similar features, with the idea that the two most similar passages are likely written by the same author.  One method for doing this is called the "standard information retrieval cosine similarity measure", which describes a non-symmetric relation between the author A (of text A) and some text B. Some research has shown that it may be more effective to measure the similarity between passages based solely on the similarity between the infrequent words (Novak), this may be because high frequency words are also those likely to be common between authors.

Although our choice of clustering algorithm may change during implementation, our initial attempts will model a process called "Variation of Information" (Meila, 2007). Variation of Information proposes a form of measure between two clusters called standard entropy and mutual information and can be used in conjunction with our cosine similarity measure.

	Moving forward, we will also want to consider the proximity of passages in assigning the likelihood of plagiarism. For example, if a single unbroken passage of text varies wildly within a document, that may be more indicative of plagiarism than if the text seems to vary back and forth from sentence to sentence in its style. Furthermore, we may want to adopt a dynamic approach to deciding where to enact passage boundaries within our analysis. We may see better results if we allow our analysis to consider the possibility that plagiarism began in the midst of a sentence, rather than only between or sentences or paragraphs. These two extensions might be aided by the implementation of a topic modeling approach, where we consider a probabilistic model of the occurrence of plagiarism (e.g. a transition probability from author one to author two).

In order to detect intrinsic plagiarism, it is necessary to have a set of features that qualify the characteristics of the text. One basic example could be the number of typos in a text--if a paper is ridden with typos except for one perfectly-typed paragraph, it may be indicative that the "good" paragraph has been taken from a different source than the author. However, given the prevalence of automated spell-checking, this stylometric feature may be of limited utility. 

	Various papers within the natural language processing community provide a wealth of information into different features for language quantification--some used specifically for plagiarism detection, some for other similar tasks.

	Meyer zu Eissen and Stein place stylometric features into five categories: character-level text statistics, sentence-level syntactic features, part-of-speech features, closed-class features (usage of specific types of words), and structural features (Meyer zu Eissen, 2007). Character-level text statistics might include punctuation counts and word lengths. Syntactic features might include sentence length, the use of function words, and more complicated concepts like the syntax-tree construction. Part-of-speech features might simply count the number of adjectives, nouns, etc. Closed-class features might include the number of prepositions or foreign words. Finally, structural features might simply measure paragraph length or similar concepts. These different statistics can be integrated to provide a quantification of the writing style of the author of a given passage.

Another technique is to examine the complexity of a piece of writing. Some of these techniques are a simple formula taking in the average number of syllables per word and the average sentence length, and outputting an estimate of the complexity (Meyer zu Eissen, 2007). While other techniques are more complicated, many share the goal of outputting a "complexity" for a piece of writing. Such an approach makes it easier to compare between two works (there is only one axis), but it is also fairly simplistic.

	More complicated systems of textual quantification do exist. The Linguistic Inquiry and Word Count (LIWC) is one such system. The LIWC attempts to judge the emotional content of text using 70 different language dimensions (Tausczik 2010). In detecting plagiarism, these additional features might provide a more nuanced categorization of the text, which could allow greater detection of differences within the document. For example, if most of a text is categorized by "positive" emotions, but one section is heavily dominated by "negative" emotions, such a disparity might suggest the occurrence of plagiarism. While the categorization of emotions may not be as salient for the detection of plagiarism, there are many other features within LIWC that might be useful.

	As we move forward in developing our plagiarism system, we will test various textual and stylometric features to decide which are most predicatory for detecting plagiarism.

	The output of our intrinsic program will be two fold, a text file and a metadata file.  The text file will contain an easy to read representation of the findings (meant for humans) while the metadata file will contain the important raw information gathered in a structured file (XML or JSON, for example). This file could then be fed into our extrinsic program or an outside resource. 

	While superfluous information regarding the gritty analysis may be left off the output text document, the file should display the clusters (of text) it created and convey why it thought that those were the correct clusters i.e., cluster A was formed because of a trend of high infrequent word use and cluster B had a low infrequent word density.  It would also be possible (maybe even desirable) to have the program print out results depending on the expected number of authors. 

The corpora to be used in testing this part of the system are described in a later section. Our approach to testing will rely on two aspects. The first is the ability to classify text consistently when plagiarism is absent, and avoid false positives. If the textual features we select vary wildly and implicate plagiarism in every piece of text, then our system will be useless for actual classification. On the other hand, if our system cannot detect changes within meaningful textual features, we will be unable to determine when a shift has potentially occurred from original to plagiarized work. 


Description of Extrinsic Program:

The other major category of plagiarism detection is extrinsic plagiarism detection.  Extrinsic plagiarism detection involves testing a document under suspicion of being plagiarized against all documents in a corpus to identify strong similarities between the documents. An obvious use case for extrinsic plagiarism detection would be in an academic setting in which a professor is interested in whether or not a student's essay, or a portion of a student's essay, is plagiarized. The fundamental difference from intrinsic detection is that the features within the suspicious document are unimportant relative to one another. Instead, the features in the suspicious document are important relative to features in external documents. In the professor example, it is crucial to identify any external documents whose features are similar to the student's essay.

	There are several different approaches to extrinsic plagiarism detection that we will be testing. In nearly every approach, the general concept will be to identify features for documents and evaluating a similarity measure between those features. A naive and intuitive approach is to generate a list of all n-grams that appear in each document and to generate a similarity measure for two documents using a set-based similarity measure (e.g. Jaccard Similarity). Of course, more sophisticated approaches exist, such as fingerprinting. Benefits to these approaches include more robust detection and better performance. One major drawback of n-gram-style approaches is the inability to detect semantic similarities between documents. It's easy to plagiarize a document by paraphrasing, but it requires an inherently different detection technique.

	Here we outline the extrinsic detection methods that we intend to explore and implement.  As a baseline, we intend to implement the naive approach as described above. It will serve as a benchmark, against which we may compare results from more sophisticated approaches. 

Next, we will explore fingerprinting techniques in great detail.  Fingerprinting is a technique used to express an entire document as a set of features.  Generally, this fingerprint is made up of several strings or hashes that are extremely small compared to the size of the original document.  A desired property for fingerprints is that if two documents share the same fingerprint, the documents should either be similar or identical. There are many methods for fingerprint generation, and each has different properties. It should be noted that no fingerprint technique is generally regarded as the best technique. There are many different parameters that can be factored into fingerprinting.  We will explore parameters such as n-gram length, fingerprint sampling count, sampling location, and the applied hash function.  A thorough comparison of fingerprinting will be created, and we will compare our results with published results.  Additionally, we will implement the Winnowing algorithm, which is a fingerprinting-related algorithm used by the plagiarism detection system MOSS (Schleimer, 2003).

The next step will be to explore paraphrasing plagiarism using a semantic-based approach. The idea behind this form of detection lies in the assumption that certain words will appear together in common phrases in the context of a given subject. An appropriate approach would be the bag-of-words model where the frequency of all the words in all the documents are counted for each individual document. High counts of the same word or the same subset of words will contribute to the similarity of two documents. These counts will be stored in a matrix of values where an entry aij will represent the count for a word i in a document j. Considering the vastness of the vocabulary that we will encounter as well as the large number of documents we will be comparing, this process will be very memory and time intensive. We plan on using Singular Value Decomposition as well as other min-hashing techniques to greatly increase the feasibility of this process (Socher, 2011).

    	A possible third step would be to investigate ranking techniques for extrinsic plagiarism detection (Hoad, 2003). This technique relies on term frequency relationships, so it might be related to the bag-of-words approach. However, this area remains relatively unexplored by us. It seems like a reliable method from the limited research we have done so far.


Description of Data:

To evaluate the quality of our plagiarism detection methods, we need a corpus (or corpora) to test them on. Six different corpora for this task already exist at http://webis.de/research/corpora and have been used in some of the literature.

PAN-PC-10
One paper, An Evaluation Framework for Plagiarism Detection describes how the PAN-PC-10 corpus was built (Potthast, 2010). In addition, the corpus includes a README giving more information about the construction and content of the data. The corpus includes a total of 27,073 documents and 65,558 plagiarism "cases" (note that documents have multiple plagiarised passages, or "cases"):
50% "source", 25% "suspicious", 25% "with plagiarism"
30% of plagiarism cases intended for intrinsic detection (i.e. their sources are NOT included in the corpus)
The cases of plagiarism were generated in two ways:
40% exact copies (note that ¾ of these cases are meant for intrinsic detection)
20% artificial (algorithmic) with "low" obfuscation
20% artificial (algorithmic) with "high" obfuscation
6% simulated (via Amazon Mechanical Turk)
14% translations (from either Spanish or German)
The source documents were clustered (k = 30) to determine various topics/classes of documents. Half of the artificial plagiarism was generated from documents within the same cluster, while the other half was generated across clusters (i.e. half of the plagiarism cases were about the same topic, which would presumably be tougher to detect than the "intra"-topic plagiarism.

In Potthast, Stein, Barron-Cedeno, and Rosso’s research, the simulated and artificial plagiarism cases are "validated" in comparison to the two corpuses mentioned below (Clough09 and METER) by comparing:
Difference between source and plagiarism in METER/Clough09
Difference between source and plagiarism in simulated/artificial cases

Clough09
This corpus consists "of short (200-300 words) answers to Computer Science questions in which plagiarism has been simulated" 19 participants created 95 answers to 5 different questions using "near copy," "light revision," "heavy revision," and non-plagiarism using 5 different wikipedia articles as source documents. The creators of the PAN-PC-10 corpus compare the two corpora in An evaluation framework for plagiarism detection (Potthast, 2010). 

METER
This corpus consists of news articles written by the Press Association (PA) wire service and derivative articles on the same events published in nine major British newspapers (Clough, 2005). The corpus is comprised of over 1,700 texts. Newspapers are free to use the PA stories verbatim. Often times PA stories are adapted to fit each papers own stylistic conventions (Clough, 2010). The files are available in an XML annotated version and each is classified as "wholly-derived," "partially-derived," or "non-derived" (Clough, 2010).  One potential issue with this corpus is that text reuse in the domain of news stories may exhibit different properties than plagiarism in an academic context (Potthast, 2004). The download link is currently broken, but contact info is provided (Clough, 2005).

Intrinsic Detection Data:
Note that the PAN-PC-10 corpus has cases specifically designed to test intrinsic detection as part of its corpus. One possible source of data, as suggested by Noah, would be to look at Wikipedia edits. Though it’s not quite exactly what we want (there are many authors as opposed to just one main author and potentially plagiarised sections), it’s worth discussing. 

Outside Queries:
With extrinsic detection, we have a source document from which we suspect the plagiarism was derived. The same is not true for intrinsic detection. Once we’ve identified potentially plagiarised sections intrinsically, it would be nice to find potential sources. The simplest way to do so would be to query a search engine (or some other outside source) for the suspicious phrase. Possible sources:

Scribd is an online library, or as they describe it: "Scribd is the world's largest digital library, where readers can discover books and written works of all kinds on the Web." It has a nice API that searches all of their public documents.
Google doesn’t appear to have an obvious "general search" API -- it looks like they only offer "custom" search which embeds Google search in a host’s website: https://developers.google.com/custom-search/
Yahoo looks like they might have a simple one: http://developer.yahoo.com/boss/search/

Foreseeable Challenges:

	Choosing which language would be best for completing all these tasks efficiently while producing effective code remains a decision to be made. We will attempt to use languages none of us have ever worked with before (C++) to help gain exposure to coding outside our norm. For many of our approaches, we will be representing our documents as vector-forms. Finding, installing, and learning to use appropriate SVM/Network Libraries for these languages may prove time-consuming and headway may not be made until this hurdle is passed. For prototyping however, SciPy and NumPy have shown to be promising vector and matrix supporting packages for Python programs. 

Timeline:
End of 8th week: Prototypes of multiple intrinsic and extrinsic algorithms working with corpora.
End of Fall Term: Intrinsic and extrinsic working together, i.e. one feeding its results into another.


References

Clough, Paul and Gaizauskas, Robert. The METER Corpus, 2005. http://nlp.shef.ac.uk/meter/
Clough, Paul, Gaizauskas, Robert, and Piao, Scott. Building and annotating a corpus of for the study of journalistic text reuse, 2002.
Clough, Paul and Stevenson, Mark. A Corpus of Plagiarised Short Answers, 2009. http://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html
Frakes, W and Baeza-Yates, R. Information Retrieval: Data Structures and Algorithms. Prentice Hall, 1992.
Gabriel, Trip. Plagiarism Lines Blur for Students in Digital Age. New York Times. 2010. http://www.nytimes.com/2010/08/02/education/02cheat.html
Hoad C., Timothy, and Zobel, Justin. Methods Identifying Versioned and Plagiarized Documents. 2003
Meila, Marina. Comparing clusterings - an information based distance, 2007.
Meyer zu Eissen, Sven and Stein, Benno. Intrinsic Plagiarism Detection, 2006. 
Meyer zu Eissen, Sven, Benno Stein, and Marion Kulig. "Plagiarism detection without reference collections." Advances in data analysis. Springer Berlin Heidelberg, 2007. 359-366.
Novak, Jasmine, Raghavan, Prabhakar, and Tomkins, Andrew. Anti-Aliasing on the Web, 2004. 
Potthast, Martin, Stein, Benno, Barron-Cedeno, Rosso, Paolo. An Evaluation Framework for Plagiarism Detection, 2010. 
Schleimer, Saul, Wilkerson, Daniel, Aiken, Alex. Winnowing: Local Algorithms for Document Fingerprinting, 2003. 
Socher, Huang, Pennington, Y. Ng, Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection, 2011.
Tausczik, Yla R., and James W. Pennebaker. "The psychological meaning of words: LIWC and computerized text analysis methods." Journal of Language and Social Psychology 29.1 (2010): 24-54.
